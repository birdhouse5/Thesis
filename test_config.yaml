# Test Configuration for Portfolio Optimization Pipeline
# This file defines different test scenarios and their parameters

test_scenarios:
  # Ultra-fast test for quick validation during development
  ultra_quick:
    max_episodes: 2
    val_episodes: 1
    test_episodes: 1
    val_interval: 1
    seq_len: 10
    min_horizon: 3
    max_horizon: 5
    batch_size: 16
    crypto_days: 2
    sp500_start: "2020-01-01"
    sp500_end: "2020-01-15"
    timeout_minutes: 5
    
  # Quick test for CI/CD pipelines
  quick:
    max_episodes: 5
    val_episodes: 2
    test_episodes: 3
    val_interval: 2
    seq_len: 20
    min_horizon: 5
    max_horizon: 10
    batch_size: 32
    crypto_days: 5
    sp500_start: "2020-01-01"
    sp500_end: "2020-02-01"
    timeout_minutes: 15
    
  # Standard test for comprehensive validation
  standard:
    max_episodes: 20
    val_episodes: 5
    test_episodes: 10
    val_interval: 5
    seq_len: 60
    min_horizon: 20
    max_horizon: 30
    batch_size: 128
    crypto_days: 10
    sp500_start: "2019-01-01"
    sp500_end: "2020-01-01"
    timeout_minutes: 45
    
  # Extended test for thorough validation
  extended:
    max_episodes: 50
    val_episodes: 10
    test_episodes: 20
    val_interval: 10
    seq_len: 100
    min_horizon: 30
    max_horizon: 50
    batch_size: 256
    crypto_days: 20
    sp500_start: "2018-01-01"
    sp500_end: "2020-01-01"
    timeout_minutes: 120

# Test matrix - defines which combinations to test
test_matrix:
  asset_classes:
    - sp500
    - crypto
  
  encoders:
    - vae
    - none
    - hmm
  
  # Minimal test combinations for CI
  ci_combinations:
    - {asset_class: sp500, encoder: vae}
    - {asset_class: crypto, encoder: none}
  
  # Full test combinations for comprehensive testing
  full_combinations:
    - {asset_class: sp500, encoder: vae}
    - {asset_class: sp500, encoder: none}
    - {asset_class: sp500, encoder: hmm}
    - {asset_class: crypto, encoder: vae}
    - {asset_class: crypto, encoder: none}
    - {asset_class: crypto, encoder: hmm}

# Environment configurations for different deployment scenarios
environments:
  local_dev:
    device: "cuda"
    mlflow_enabled: true
    log_level: "DEBUG"
    parallel_tests: false
    
  ci_cd:
    device: "cpu"
    mlflow_enabled: false
    log_level: "INFO"
    parallel_tests: true
    timeout_factor: 2.0
    
  staging:
    device: "cuda"
    mlflow_enabled: true
    log_level: "INFO"
    parallel_tests: true
    
  production:
    device: "cuda"
    mlflow_enabled: true
    log_level: "WARNING"
    parallel_tests: false
    run_extended_tests: true

# Expected performance thresholds (for regression testing)
performance_thresholds:
  training_speed:
    min_episodes_per_minute: 10  # Minimum episodes/minute for quick mode
    max_memory_mb: 4096          # Maximum memory usage in MB
    
  model_performance:
    min_sharpe_ratio: -2.0       # Minimum acceptable Sharpe ratio
    max_drawdown: 0.8           # Maximum acceptable drawdown (80%)
    min_win_rate: 0.3           # Minimum win rate (30%)
    
  system_requirements:
    min_python_version: "3.8"
    required_packages:
      - torch>=1.12.0
      - numpy>=1.21.0
      - pandas>=1.3.0
      - scikit-learn>=1.0.0
      - mlflow>=2.0.0

# Smoke test specific configurations
smoke_test:
  # Tests to always run
  critical_tests:
    - imports
    - sp500_dataset_creation
    - environment_creation
    - model_creation_vae
    - training_loop_vae
    
  # Tests that can be skipped in emergency situations
  optional_tests:
    - crypto_dataset_creation
    - model_creation_hmm
    - training_loop_hmm
    - mlflow_integration
    
  # Error tolerance settings
  error_handling:
    allow_mlflow_failures: true      # MLflow failures don't fail entire test
    allow_crypto_failures: false     # Crypto failures do fail the test
    retry_failed_tests: 1            # Number of retries for failed tests
    continue_on_non_critical: true   # Continue if non-critical tests fail

# Reporting and notifications
reporting:
  formats:
    - console
    - json
    - html
    
  outputs:
    console_summary: true
    detailed_log: true
    junit_xml: false
    html_report: false
    
  notifications:
    on_failure:
      - log
      - email  # if configured
    on_success:
      - log