Implementations:

- A2C (optional)
- noise control
- early stopping
- backtesting (including control against noise?) -> find research perspectives
- benchmarking (run benchmarking using multiple N whenever non-deterministic)
    - buy and hold
    - random
    - Mean-variance optimization (Markowitz)
    - MetaTrader approaches:    [17] Narasimhan Jegadeesh and Sheridan Titman. 1993. Returns to Buying Winners
                                    and Selling Losers: Implications for Stock Market Efficiency. The Journal of
                                    Finance 48, 1 (1993), 65–91. https://doi.org/10.1111/j.1540-6261.1993.tb04702.x
                                [18] Narasimhan Jegadeesh and Sheridan Titman. 2015. Cross-Sectional and
                                    Time-Series Determinants of Momentum Returns. The Review of Finan-
                                    cial Studies 15, 1 (06 2015), 143–157. https://doi.org/10.1093/rfs/15.1.143
- ablation
- sharpe vs. max drawdown (or other metric) or DSR
- transaction cost yes and no





---


Environment:
- only SNP500
- additional technical indicators or none
- market structure or none
- increase number of asset features and market features. Start with simple measures.
- implement Sharpe ratio for now and test DSR later. Metatrader has a different learning setup, our might not require DSR (no sparse rewar problem due to rollout strategy)
- we allow cash positions for now but will punish with inflation cost later

Action space:
- leverage
- cash position?
- fixed vs. continuous allocation
- we start with no shorting and include later, including Total capital constraint (often called a gross exposure or leverage constraint)


Task sampling:
- random time window
- trajectory length
- create volatility clusters as tasks? Compare against temporal sampling
- experiment with different H and N: start with H = 60 days and N = 4, try smaller H and N and bigger ones and compare performance.
- How quickly to adapt the VAE / Policy might also be interesting.

VAE:
- how to do the pre-collection
- pre-train the VAE? (in variBAD: " In practice, we may subsample a
fixed number of ELBO terms (for random time steps t) for computational efficiency if H+ is large.")
- encoder: GRU or LSTM, vary hidden size and other parameters. Potentially try out other architectures (e.g.  Zaheer et al. (2017); Garnelo
et al. (2018); Rakelly et al. (2019))
- perhaps involve some kind of convolutional net like in metatrader

Policy:
- use variBAD policy networks first, then apply Temporal Attention from metatrader. Compare
- in variBAD the policy in conditioned on the current state. However since our portfolio context involved time series,
    we perhaps should encode state information a bit differently. In fact, we already do: we include SMA's and other technical
    indicators and market state in the state space. Perhaps we should try to also involve some kind of temporal convolution
    somewhere to see if there is improvement.
- try large vs. small buffer size: variance-bias trade-off
- try Portfolio policy using Dirichlet distribution for portfolio weights. (More principled than categorical approximation but computationally heavier)
- explain why we use PPO and not A2C (efficiency, stability, etc.) in the paper



Graphs for Paper:
- visualize the results of the first optuna run ()
- compare the mean,var over sharpe ratios over all runs?


Benchmarks:
- random allocation
- simple markowitz
- buy and hold
- RL
- metatrader implementation?

