
-> re-introduce early-stopping
-> build comprehensive .csv-logging (one row per episode?)

-> try different reward functions:
        Maximum Drawdown (MDD)
        sharpe
-> fix crypto [done]
-> remove checkpoints? [done]
-> run hmm study [done]
-> debug -inf validation performance [done]
-> run HPO successfully [done]
-> clear up performance issues related to transaction cost and inflation [done]

debug
-> metrics should be more transparent: standalone log file for training, validation and backtest. [done]
-> make sure that weights are captured [done]
-> double check the metrics
-> what are these: recent_avg_episode_reward,recent_std_episode_reward,recent_avg_policy_loss,recent_avg_vae_loss, total_steps?

-> early stopping does not kill the run [done]

-> justify every implementation detail in paper?

--

- context_len != None
- visualize the VAE output to track regime/context predictions
- check value = 0.0 for some metrics
- find other things to track / do differently. Go through the paper systematically and find mismatches between description and implementation

- implement hmm / volatility based regime sampling and compare to current implementation (random task sampling)
- check value_loss and entropy
- use approximately the same horizon length as during training