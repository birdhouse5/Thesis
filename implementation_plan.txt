
-> re-introduce early-stopping
-> build comprehensive .csv-logging (one row per episode?)

-> try different reward functions:
        Maximum Drawdown (MDD)
        sharpe
-> fix crypto
-> remove checkpoints?
-> run hmm study
-> debug -inf validation performance [done]
-> run HPO successfully [done]
-> clear up performance issues related to transaction cost and inflation [done]

debug
-> metrics should be more transparent: standalone log file for training, validation and backtest.
-> make sure that weights are captured
-> double check the metrics
-> what are these: recent_avg_episode_reward,recent_std_episode_reward,recent_avg_policy_loss,recent_avg_vae_loss, total_steps?

-> early stopping does not kill the run [done]

-> justify every implementation detail in paper?